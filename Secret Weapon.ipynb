{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1v3RbPf13uXN",
        "outputId": "c2abccb6-309a-4ac2-c1c4-1ed9be363029"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-SdAcaPn_fYy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"sk-3cc94a27441a472395f00ba59dd1634f\",\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "\n",
        "def format_history(chat_history):\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "    for msg in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": msg[0]})\n",
        "        if msg[1]:\n",
        "            final_reply = msg[1].split(\"ÊúÄÁªàÂõûÂ§çÔºö\")[-1].strip()\n",
        "            messages.append({\"role\": \"assistant\", \"content\": final_reply})\n",
        "    return messages\n",
        "\n",
        "def predict(message, chat_history):\n",
        "    chat_history.append((message, \"\"))\n",
        "    messages = format_history(chat_history[:-1])\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen3-235b-a22b\",\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "        stream_options={\"include_usage\": True},\n",
        "        extra_body={\"enable_thinking\": True},\n",
        "    )\n",
        "\n",
        "    full_response = \"\"\n",
        "    full_reasoning = \"\"\n",
        "    current_reasoning = \"\"\n",
        "    for chunk in completion:\n",
        "        chunk_data = chunk.model_dump()\n",
        "        if not chunk_data.get('choices') or len(chunk_data['choices']) == 0:\n",
        "          continue  \n",
        "        delta = chunk_data['choices'][0]['delta']\n",
        "\n",
        "        if delta.get('reasoning_content'):\n",
        "            current_reasoning = delta['reasoning_content']\n",
        "            full_reasoning += current_reasoning\n",
        "\n",
        "        if delta.get('content'):\n",
        "            full_response += delta['content']\n",
        "\n",
        "        display_text = \"\"\n",
        "        if full_reasoning != '':\n",
        "            display_text = \"üß† ThinkingÔºö\\n\" + full_reasoning\n",
        "        if full_response:\n",
        "            display_text += f\"\\nüí° ResponseÔºö\\n{full_response}\"\n",
        "\n",
        "        if current_reasoning or delta.get('content'):\n",
        "            chat_history[-1] = (message, display_text)\n",
        "            yield chat_history\n",
        "\n",
        "        if delta.get('content'):\n",
        "            current_reasoning = \"\"\n",
        "\n",
        "css = \"\"\"\n",
        ".gr-chatbot {min-height: 600px;}\n",
        ".gr-chatbot .assistant-message {white-space: pre-wrap;}\n",
        ".dark .reasoning {color: #90CAF9;}\n",
        ".reasoning {color: #1E88E5; font-style: italic;}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"# <center>Test</center>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(\n",
        "            bubble_full_width=False,\n",
        "            render_markdown=True,\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(\n",
        "            placeholder=\"Question...\",\n",
        "            container=False,\n",
        "            scale=5\n",
        "        )\n",
        "        clear = gr.Button(\"üóëÔ∏è Clear\", scale=1)\n",
        "\n",
        "    msg.submit(predict, [msg, chatbot], chatbot)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_name=\"0.0.0.0\",debug=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
